services:
  startup:
    extends:
      file: common.yml
      service: oada-uservice
    image: oada/startup:${OADA_VERSION-build}
    build:
      args:
        OADA_SERVICE: startup
    depends_on:
      - kafka
      - arangodb
    expose: # expose only internally, not on host
      - '8080'
    environment:
      # set RESETDATABASE to "yes" if you want to drop database on startup and recreate
      - RESETDATABASE=${RESETDATABASE-no}

  auth:
    extends:
      file: common.yml
      service: oada-uservice
    image: oada/auth:${OADA_VERSION-build}
    build:
      args:
        OADA_SERVICE: auth
    depends_on:
      - startup
    networks:
      - http_net
    expose: # expose only internally, not on host
      - '8080'

  # http-handler is in charge of maintaining connectiongs to clients and starting
  # the first message for a request into Kafka
  http-handler:
    extends:
      file: common.yml
      service: oada-uservice
    image: oada/http-handler:${OADA_VERSION-build}
    build:
      args:
        OADA_SERVICE: http-handler
    depends_on:
      - startup
    networks:
      - http_net
    expose: # expose only internally, not on host
      - '8080'
    environment:
      - IGNORE_SCOPE
    volumes:
      # Storage for non-JSON resources
      - binary_data:/oada/binary

  sync-handler:
    extends:
      file: common.yml
      service: oada-uservice
    image: oada/sync-handler:${OADA_VERSION-build}
    build:
      args:
        OADA_SERVICE: sync-handler
    depends_on:
      - startup
      - proxy
    networks:
      - http_net
    environment:
      - IGNORE_SCOPE

  write-handler:
    extends:
      file: common.yml
      service: oada-uservice
    image: oada/write-handler:${OADA_VERSION-build}
    build:
      args:
        OADA_SERVICE: write-handler
    depends_on:
      - startup

  users:
    extends:
      file: common.yml
      service: oada-uservice
    image: oada/users:${OADA_VERSION-build}
    build:
      args:
        OADA_SERVICE: users
    depends_on:
      - startup

  rev-graph-update:
    extends:
      file: common.yml
      service: oada-uservice
    image: oada/rev-graph-update:${OADA_VERSION-build}
    build:
      args:
        OADA_SERVICE: rev-graph-update
    depends_on:
      - startup

  well-known:
    extends:
      file: common.yml
      service: oada-uservice
    image: oada/well-known:${OADA_VERSION-build}
    build:
      args:
        OADA_SERVICE: well-known
    depends_on:
      - startup
    networks:
      - http_net
    expose: # expose only internally, not on host
      - '8080'

  webhooks:
    extends:
      file: common.yml
      service: oada-uservice
    image: oada/webhooks:${OADA_VERSION-build}
    build:
      args:
        OADA_SERVICE: webhooks
    depends_on:
      - startup
      - proxy
    networks:
      - http_net
    environment:
      - SSL_ALLOW_SELF_SIGNED=1

  shares:
    extends:
      file: common.yml
      service: oada-uservice
    image: oada/shares:${OADA_VERSION-build}
    build:
      args:
        OADA_SERVICE: shares
    depends_on:
      - startup

  # swag with configs for auth, well-known, and http-handler
  proxy:
    build: ./support/proxy
    image: oada/support-proxy:${OADA_VERSION-build}
    cap_add:
      - NET_ADMIN
    networks:
      - http_net
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=America/New_york
      - URL=${DOMAIN:-localhost}
      - SUBDOMAINS=www,
      - VALIDATION=http
      - CERTPROVIDER= #optional
      - DNSPLUGIN= #optional
      - PROPAGATION= #optional
      - DUCKDNSTOKEN= #optional
      - EMAIL= #optional
      - ONLY_SUBDOMAINS=false #optional
      - EXTRA_DOMAINS= #optional
      - STAGING=true #optional (set false for release)
      - MAXMINDDB_LICENSE_KEY= #optional
    volumes:
      # Hack in self-signed ssl for localhost
      - ./support/proxy/selfsigned:/config/keys/letsencrypt:ro
    ports:
      - '${BIND:-0.0.0.0}:${PORT_HTTPS:-443}:443'
      - '${BIND:-0.0.0.0}:${PORT_HTTP:-80}:80'
    restart: unless-stopped

  # Arango is the main backend where core data and graph is stored
  arangodb:
    image: arangodb:3.7.10
    restart: unless-stopped
    networks:
      - arango_net
    volumes:
      - arangodb_data:/var/lib/arangodb3
      - arangodb_apps_data:/var/lib/arangodb3-apps
    expose: # expose only internally, not on host
      - '8529'
    environment:
      # - ARANGO_RANDOM_ROOT_PASSWORD=1
      - ARANGO_NO_AUTH=1
      - ARANGO_STORAGE_ENGINE=rocksdb
      - ARANGO_STATISTICS=0
    #command: ['arangod', '--server.statistics', 'true', '--database.auto-upgrade', 'true']
    command: ['arangod', '--server.statistics', 'true']

  # Redpanda is API compatible with kafka.
  # The name is still kafka to not break old code.
  kafka:
    image: vectorized/redpanda:v21.4.13
    restart: unless-stopped
    networks:
      - kafka_net
    volumes:
      - redpanda_data:/var/lib/redpanda/data
      - redpanda_config:/etc/redpanda/
    environment:
      REDPANDA_MODE: ${REDPANDA_MODE-development}
    entrypoint:
      - /bin/sh
      - -c
    # Configure then run redpanda?
    # TODO: Better way to set this?
    command:
      - |
        rpk redpanda mode $$REDPANDA_MODE &&
        rpk redpanda start \
          --smp '1' \
          --reserve-memory 0M \
          --overprovisioned \
          --kafka-addr PLAINTEXT://0.0.0.0:9092 \
          --rpc-addr 0.0.0.0:33145 \
          --node-id '0' \
          --advertise-kafka-addr PLAINTEXT://kafka:9092
    expose:
      # Kafka API port
      - '9092'
      # Internal RPC port
      - '33145'
      # Prometheus/HTTP Admin port
      - '9644'

  prometheus:
    image: prom/prometheus
    profiles:
      - dashboard
      - debug
    depends_on:
      - rpk-prometheus-conf
      - kafka
    restart: unless-stopped
    networks:
      - grafana_net
      - kafka_net
    expose:
      - '9090'
    #ports:
    #  - '9090:9090'
    volumes:
      - prometheus_data:/prometheus
      - prometheus_config:/etc/prometheus

  # Generate redpanda config for prometheus
  # TODO: Better way to do this?
  rpk-prometheus-conf:
    extends:
      service: kafka
    profiles:
      - dashboard
      - debug
    restart: on-failure
    volumes:
      - prometheus_config:/etc/prometheus
    user: root
    entrypoint:
      - '/bin/sh'
      - -c
    command:
      - |
        echo -n "scrape_configs:" > /etc/prometheus/prometheus.yml &&
        rpk generate --seed-addr=kafka prometheus-config >> /etc/prometheus/prometheus.yml

  grafana:
    image: grafana/grafana
    profiles:
      - dashboard
      - debug
    depends_on:
      - prometheus
      - grafana-provision-prometheus
      - grafana-provision-dashboards
    restart: unless-stopped
    networks:
      - grafana_net
    ports:
      - '3000:3000'
    volumes:
      - grafana_data:/var/lib/grafana
      - grafana_prov:/etc/grafana/provisioning

  # Provision a grafana dashboard provider.
  #
  # Loads all dashboards in /var/lib/grafana/dashboards
  #
  # TODO: Better way to do this?
  grafana-provision-dashboards:
    image: grafana/grafana
    profiles:
      - dashboard
      - debug
    depends_on:
      - rpk-grafana-dashboard
    restart: on-failure
    volumes:
      - grafana_prov:/etc/grafana/provisioning
    entrypoint:
      - '/bin/sh'
      - -c
    command:
      - |
        echo '
        apiVersion: 1
        providers:
          - name: "dashboard provider"
            type: file
            options:
              path: /var/lib/grafana/dashboards
              foldersFromFileStructure: true
        ' > /etc/grafana/provisioning/dashboards/dashboards.yml

  # Provision our prometheus instance as a grafana datasource
  # TODO: Better way to do this?
  grafana-provision-prometheus:
    image: grafana/grafana
    profiles:
      - dashboard
      - debug
    restart: on-failure
    volumes:
      - grafana_prov:/etc/grafana/provisioning
    entrypoint:
      - '/bin/sh'
      - -c
    command:
      - |
        echo '
        apiVersion: 1
        deleteDatasources:
          - name: Prometheus
        datasources:
          - name: Prometheus
            type: prometheus
            access: proxy
            url: http://prometheus:9090
        ' > /etc/grafana/provisioning/datasources/prometheus.yml

  # Generate dashboard for our redpanda with our prometheus instance
  # TODO: Better way to do this?
  rpk-grafana-dashboard:
    extends:
      service: kafka
    profiles:
      - dashboard
      - debug
    restart: on-failure
    volumes:
      - grafana_data:/var/lib/grafana
      - grafana_prov:/etc/grafana/provisioning
    entrypoint:
      - '/bin/sh'
      - -c
    command:
      - |
        mkdir -p /var/lib/grafana/dashboards/
        rpk generate grafana-dashboard \
          --datasource Prometheus \
          --prometheus-url kafka:9644/metrics \
          > /var/lib/grafana/dashboards/redpanda.json

volumes:
  arangodb_data:
  arangodb_apps_data:
  binary_data:
  redpanda_data:
  redpanda_config:
  prometheus_data:
  prometheus_config:
  grafana_data:
  grafana_prov:

networks:
  arango_net:
  kafka_net:
  http_net:
  startup_net:
  grafana_net:

# Set up Mutagen forwards
x-mutagen:
  forward:
    agrango:
      source: 'tcp:localhost:8529'
      destination: 'network://arango_net:tcp:arangodb:8529'
    kafka:
      source: 'tcp:localhost:9092'
      destination: 'network://kafka_net:tcp:kafka:9092'
    prometheus:
      source: 'tcp:localhost:9090'
      destination: 'network://grafana_net:tcp:prometheus:9090'
    grafana:
      source: 'tcp:localhost:3000'
      destination: 'network://grafana_net:tcp:grafana:3000'
