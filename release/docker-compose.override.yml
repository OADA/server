# Copyright 2021 Open Ag Data Alliance
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

x-release:
  environment: &env # Default to loading packaged default config?
    CONFIG: ${OADA_CONFIG-/oada.config.js}
    NODE_ENV: ${NODE_ENV-production}
    DEBUG: ${DEBUG-*:error,*:warn,*:info}
    PINO_LEVEL:
    DEBUG_HIDE_DATE:
    DEBUG_COLORS:
    DEBUG_DEPTH:
    DEBUG_SHOW_HIDDEN:
    PINO_TRANSPORT: ${PINO_TRANSPORT-yarn g:pretty -clti pid,hostname}
    DOMAIN:
    NODE_TLS_REJECT_UNAUTHORIZED:
    # Allow changing arangodb URL (e.g., to use external arango)
    ARANGODB_URL: ${ARANGODB_URL-http://arangodb:8529}
    # Allow changing kafka brokers (e.g., to use external kafka)
    KAFKA_BROKERS: ${KAFKA_BROKERS-kafka:9092}

services:
  startup:
    environment:
      <<: *env
      # set RESETDATABASE to "yes" if you want to drop database on startup
      # and recreate it
      RESETDATABASE: ${RESETDATABASE-no}

  auth:
    environment:
      <<: *env

  http-handler:
    environment:
      <<: *env
      IGNORE_SCOPE:
    volumes:
      # Storage for non-JSON resources
      - binary_data:/oada/binary

  sync-handler:
    environment:
      <<: *env
      IGNORE_SCOPE:

  write-handler:
    environment:
      <<: *env

  users:
    environment:
      <<: *env

  rev-graph-update:
    environment:
      <<: *env

  well-known:
    environment:
      <<: *env

  webhooks:
    environment:
      <<: *env

  shares:
    environment:
      <<: *env

  # swag with configs for auth, well-known, and http-handler
  #
  # Handles reverse proxying to services and SSL certs.
  #
  # Can be replaced with any other reverse proxy solution.
  # Needs to route:
  #   /oadaauth -> auth
  #   /.well-known -> well-known
  #   / -> http-handler websockets
  #   all other OADA requestes -> http-handler
  proxy:
    #volumes:
    #  To disable fetching SSL certs (e.g. to serve localhost)
    #  Generate your own fullchain.pem and privkey.pem
    #  put them into /config/keys/letsencrypt (see below)
    #  - /path/to/certs:/config/keys/letsencrypt:ro
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=America/New_york
      - URL=${DOMAIN}
      - SUBDOMAINS=, #e.g., www
      - VALIDATION=http
      - CERTPROVIDER= #optional
      - DNSPLUGIN= #optional
      - PROPAGATION= #optional
      - DUCKDNSTOKEN= #optional
      - EMAIL= #optional
      - ONLY_SUBDOMAINS=false #optional
      - EXTRA_DOMAINS= #optional
      - STAGING=false #optional
      - MAXMINDDB_LICENSE_KEY= #optional
    volumes:
      - proxy_config:/config
    ports:
      - '${BIND:-0.0.0.0}:${PORT_HTTPS:-443}:443'
      - '${BIND:-0.0.0.0}:${PORT_HTTP:-80}:80'

  # Arango is the main backend where core data and graph are stored
  #
  # You could use an external arango instance instead.
  # The services just need hostname `arango` to resolve to arango instance(s)
  arangodb:
    volumes:
      - arangodb_data:/var/lib/arangodb3
      - arangodb_apps_data:/var/lib/arangodb3-apps
    environment:
      # - ARANGO_RANDOM_ROOT_PASSWORD=1
      - ARANGO_NO_AUTH=1
      - ARANGO_STORAGE_ENGINE=rocksdb

  # You could use an external kafka instance instead.
  # The services just need hostname `kafka`
  # to resolve to redpanda/kafka instance(s)
  redpanda:
    environment:
      REDPANDA_MODE: ${REDPANDA_MODE-production}
    volumes:
      #- /var/run/docker.sock:/var/run/docker.sock
      - redpanda_data:/var/lib/redpanda/data
      - redpanda_config:/etc/redpanda/

  prometheus:
    volumes:
      - prometheus_data:/prometheus
      - prometheus_config:/etc/prometheus

  rpk-prometheus-conf:
    volumes:
      - prometheus_config:/etc/prometheus

  grafana:
    volumes:
      - grafana_data:/var/lib/grafana
      - grafana_prov:/etc/grafana/provisioning

  grafana-provision-dashboards:
    volumes:
      - grafana_prov:/etc/grafana/provisioning

  grafana-provision-prometheus:
    volumes:
      - grafana_prov:/etc/grafana/provisioning

  rpk-grafana-dashboard:
    volumes:
      - grafana_data:/var/lib/grafana
      - grafana_prov:/etc/grafana/provisioning

volumes:
  arangodb_data:
  arangodb_apps_data:
  binary_data:
  redpanda_data:
  redpanda_config:
  proxy_config:
  prometheus_data:
  prometheus_config:
  grafana_data:
  grafana_prov:

# Set up Mutagen forwards
# Only take effect if using `mutagen compose` rather than `docker-compose`
# see https://mutagen.io/documentation/orchestration/compose
x-mutagen:
  forward:
    agrango:
      source: 'tcp:localhost:8529'
      destination: 'network://arango_net:tcp:arangodb:8529'
    kafka:
      source: 'tcp:localhost:9092'
      destination: 'network://kafka_net:tcp:kafka:9092'
    prometheus:
      source: 'tcp:localhost:9090'
      destination: 'network://grafana_net:tcp:prometheus:9090'
    grafana:
      source: 'tcp:localhost:3000'
      destination: 'network://grafana_net:tcp:grafana:3000'
